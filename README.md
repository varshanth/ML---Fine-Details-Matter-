# Machine Learning: Fine Details Matter!

Understanding machine (and deep) learning concepts from can be challenging. It can be even more challenging to people who just can't accept stated facts which they don't understand. Below is a list of resources that I referred to when I was seeking for the "right" words/representations to comprehend certain topics. The list certainly isn't exhaustive nor is it meant to cover the basic understanding of any concept (there are abundant resources for that). In my opinion, the effectiveness of a resource is, how convincing it can be to those people who cannot easily get convinced. I sincerely thank the authors of these resources as they have made me comprehend conceptual fragments (if not the entirety) and hence cementing my foundation in these topics.

## Machine Learning

### Winnow & Perceptron:
* J. Kivinen, M.K. Warmuth, P. Auer, The perceptron algorithm versus winnow: linear versus logarithmic mistake bounds when few input variables are relevant, In Artificial Intelligence, Volume 97, Issues 1â€“2, 1997, Pages 325-343, ISSN 0004-3702, https://doi.org/10.1016/S0004-3702(97)00039-8. (http://www.sciencedirect.com/science/article/pii/S0004370297000398)

* (Other topics also covered here) SIMS 290-2: Applied Natural Language Processing: Marti Hearst & Barbara Rosario: courses.ischool.berkeley.edu/i256/f06/lectures/lecture17.ppt

* 8803 Machine Learning Theory. Maria-Florina Balcan: The Winnow Algorithm - www.cs.cmu.edu/~ninamf/ML11/lect0906.pdf

### Linear, Lasso and Ridge Regression
* Linear vs Lasso vs Ridge: https://discuss.analyticsvidhya.com/t/comparison-between-ridge-linear-and-lasso-regression/8213
* Machine Learning Thoughts: When does sparsity occur: ml.typepad.com/machine_learning_thoughts/2005/11/when_does_spars.html
* Excellent explanation of L1 vs L2 Regularization by Prof. Alexander Ihler:  https://www.youtube.com/watch?v=sO4ZirJh9ds&list=PLkWzaBlA7utJMRi89i9FAKMopL0h0LBMk&index=16
* Visualizing Norms as a unit circle: https://www.youtube.com/watch?v=SXEYIGqXSxk
* Proximal Operator as the Shrinkage Operator in Soft Thresholding Algorithms: http://www.onmyphd.com/?p=proximal.operator
* Derivation of the soft thresholding operator: https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator
* Diffentiable criteria: When can I say a function is differentiable? Useful for the understanding the soft thresholding operator -  https://www.mathsisfun.com/calculus/differentiable.html

## Logistic Regression
* Bernoulli Distribution: https://en.wikipedia.org/wiki/Bernoulli_distribution
* Logits: https://stats.stackexchange.com/questions/52825/what-does-the-logit-value-actually-mean
* Logits & Log-odds: https://stats.idre.ucla.edu/stata/faq/how-do-i-interpret-odds-ratios-in-logistic-regression/
* Likelihood Function for Logistic Regression: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf
* Simple Derivation of Logistic Regression: http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/

 




