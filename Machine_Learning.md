## Machine Learning

### Winnow & Perceptron:
* J. Kivinen, M.K. Warmuth, P. Auer, The perceptron algorithm versus winnow: linear versus logarithmic mistake bounds when few input variables are relevant, In Artificial Intelligence, Volume 97, Issues 1â€“2, 1997, Pages 325-343, ISSN 0004-3702, https://doi.org/10.1016/S0004-3702(97)00039-8. (http://www.sciencedirect.com/science/article/pii/S0004370297000398)

* (Other topics also covered here) SIMS 290-2: Applied Natural Language Processing: Marti Hearst & Barbara Rosario: courses.ischool.berkeley.edu/i256/f06/lectures/lecture17.ppt

* 8803 Machine Learning Theory. Maria-Florina Balcan: The Winnow Algorithm - www.cs.cmu.edu/~ninamf/ML11/lect0906.pdf

### Linear, Lasso and Ridge Regression
* Linear vs Lasso vs Ridge: https://discuss.analyticsvidhya.com/t/comparison-between-ridge-linear-and-lasso-regression/8213
* Machine Learning Thoughts: When does sparsity occur: ml.typepad.com/machine_learning_thoughts/2005/11/when_does_spars.html
* Excellent explanation of L1 vs L2 Regularization by Prof. Alexander Ihler:  https://www.youtube.com/watch?v=sO4ZirJh9ds&list=PLkWzaBlA7utJMRi89i9FAKMopL0h0LBMk&index=16
* Visualizing Norms as a unit circle: https://www.youtube.com/watch?v=SXEYIGqXSxk
* Proximal Operator as the Shrinkage Operator in Soft Thresholding Algorithms: http://www.onmyphd.com/?p=proximal.operator
* Derivation of the soft thresholding operator: https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator
* Diffentiable criteria: When can I say a function is differentiable? Useful for the understanding the soft thresholding operator -  https://www.mathsisfun.com/calculus/differentiable.html

## Logistic Regression
* Bernoulli Distribution: https://en.wikipedia.org/wiki/Bernoulli_distribution
* Logits: https://stats.stackexchange.com/questions/52825/what-does-the-logit-value-actually-mean
* Logits & Log-odds: https://stats.idre.ucla.edu/stata/faq/how-do-i-interpret-odds-ratios-in-logistic-regression/
* Likelihood Function for Logistic Regression: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf
* Simple Derivation of Logistic Regression: http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/

### Basic Neural Networks
* Just words: http://www.explainthatstuff.com/introduction-to-neural-networks.html
* Words with a bit of math: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
* Backprop:
** Intuition: https://www.youtube.com/watch?v=Ilg3gGewQ5U
** Intuition converting to maths: https://www.youtube.com/watch?v=An5z8lR8asY
** Backprop maths: https://www.youtube.com/watch?v=gl3lfL-g5mA
** More Backprop maths: https://www.youtube.com/watch?v=aVId8KMsdUU
