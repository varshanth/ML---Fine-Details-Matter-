## Machine Learning
These resources are not mine and I do not claim that I have written them. All the resources listed here can be found publicly.

### Winnow & Perceptron:
* J. Kivinen, M.K. Warmuth, P. Auer, The perceptron algorithm versus winnow: linear versus logarithmic mistake bounds when few input variables are relevant, In Artificial Intelligence, Volume 97, Issues 1â€“2, 1997, Pages 325-343, ISSN 0004-3702, https://doi.org/10.1016/S0004-3702(97)00039-8. (http://www.sciencedirect.com/science/article/pii/S0004370297000398)

* (Other topics also covered here) SIMS 290-2: Applied Natural Language Processing: Marti Hearst & Barbara Rosario: courses.ischool.berkeley.edu/i256/f06/lectures/lecture17.ppt

* 8803 Machine Learning Theory. Maria-Florina Balcan: The Winnow Algorithm - www.cs.cmu.edu/~ninamf/ML11/lect0906.pdf

### Linear, Lasso and Ridge Regression
* Linear vs Lasso vs Ridge: https://discuss.analyticsvidhya.com/t/comparison-between-ridge-linear-and-lasso-regression/8213
* Machine Learning Thoughts: When does sparsity occur: www.ml.typepad.com/machine_learning_thoughts/2005/11/when_does_spars.html
* Excellent explanation of L1 vs L2 Regularization by Prof. Alexander Ihler:  https://www.youtube.com/watch?v=sO4ZirJh9ds&list=PLkWzaBlA7utJMRi89i9FAKMopL0h0LBMk&index=16
* Visualizing Norms as a unit circle: https://www.youtube.com/watch?v=SXEYIGqXSxk
* Proximal Operator as the Shrinkage Operator in Soft Thresholding Algorithms: http://www.onmyphd.com/?p=proximal.operator
* Derivation of the soft thresholding operator: https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator
* Diffentiable criteria: When can I say a function is differentiable? Useful for the understanding the soft thresholding operator:
https://www.mathsisfun.com/calculus/differentiable.html

### Logistic Regression
* Bernoulli Distribution: https://en.wikipedia.org/wiki/Bernoulli_distribution
* Logits: https://stats.stackexchange.com/questions/52825/what-does-the-logit-value-actually-mean
* Logits & Log-odds: https://stats.idre.ucla.edu/stata/faq/how-do-i-interpret-odds-ratios-in-logistic-regression/
* Likelihood Function for Logistic Regression: Prof. Cosma Shalizi's 2012 Lecture: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf
* Simple Derivation of Logistic Regression: http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/

### Nearest Neighbors
* The only resource you will ever need to understand: Prof. Victor Lavrenko's: https://www.youtube.com/playlist?list=PLBv09BD7ez_68OwSB97WXyIOvvI5nqi-3
* Mahalanobis Distance:
1. https://www.youtube.com/watch?v=3IdvoI8O9hU
2. https://www.youtube.com/watch?v=spNpfmWZBmg

### Hard & Soft Margin Support Vector Machines & KKT Conditions
Warning: These links might contain kernel concepts which are covered in the next section. So you might want to ignore kernels until you read that.
* Prof. Alexander Ihler
1. Part 1: https://www.youtube.com/watch?v=IOetFPgsMUc
2. Part 2: https://www.youtube.com/watch?v=1aQLEzeGJC8
* Prof. Patrick Winston: https://www.youtube.com/watch?v=_PwhiWxHK8o 

* MLPR: SVM by Prof. Coryn Bailer-Jones: http://www.mpia.de/homes/calj/ss2007_mlpr/course/support_vector_machines.odp.pdf
* A bit complicated but if you have seen the above, you will understand this: A Tutorial on SVM for Pattern Recognition by Christopher J.C Burges: www.cmap.polytechnique.fr/~mallat/papiers/svmtutorial.pdf

* Advanced: v-SVM: A Tutorial on v-SVM by Chen et. al: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.407.9879&rep=rep1&type=pdf

### Kernels
* Prof. Alexander Ihler: https://www.youtube.com/watch?v=OmTu0fqUsQk
* The Kernel Trick - Udacity : https://www.youtube.com/watch?v=N_r9oJxSuRs
* Representer Theorem, Kernel Examples and Proofs by Prof. Peter Bartlett: https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/8.pdf
*  Positive Definite Kernels, RKHS, Representer Theorem: NPTEL - Prof. P.S Sastry : https://www.youtube.com/watch?v=_dyUl_luJl4

### Basic Feedforward Neural Networks
* Just words: http://www.explainthatstuff.com/introduction-to-neural-networks.html
* Words with a bit of math: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/

#### Backpropagation
* Intuition: https://www.youtube.com/watch?v=Ilg3gGewQ5U
* Intuition converting to maths: https://www.youtube.com/watch?v=An5z8lR8asY
* Backprop maths: https://www.youtube.com/watch?v=gl3lfL-g5mA
* More Backprop maths: https://www.youtube.com/watch?v=aVId8KMsdUU
